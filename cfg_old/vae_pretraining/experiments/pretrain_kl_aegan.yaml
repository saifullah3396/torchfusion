# @package _global_
defaults:
  - /args/model_args: '???'
  - /args/data_args: '???'
  - override /args/training_args: default

args:
  general_args:
    do_train: ${do_train}
    do_val: ${do_val}
    do_test: ${do_test}
    n_devices: 1
  training_args:
    experiment_name: pretrain_vaegan
    max_epochs: 1
    eval_on_start: True
    eval_every_n_epochs: 0.1
    visualize_every_n_epochs: 0.1
    visualize_on_start: False
    test_run: False
    with_amp: False
    with_amp_inference: False
    stop_on_nan: True
    eval_training: false # we don't want to evaluate on training set since this is generative task and fid is applied only to eval set
    use_ema_for_val: True
    model_ema_args:
      enabled: True
  data_args:
    dataset_config_name: ${dataset_config_name}
    data_loader_args:
      dataloader_num_workers: 0
      per_device_train_batch_size: 1
      per_device_eval_batch_size: 1
      max_val_samples: ${args.data_args.dataset_statistics_n_samples} # validation sample must stay equal to dataset_statistics_n_samples
  model_args:
    model_config:
      disc_start: 10001
      kl_weight: 0.000001
      disc_weight: 0.5
      model_constructor_args:
        checkpoint: ${checkpoint}
        checkpoint_state_dict_key: ${checkpoint_state_dict_key}
        checkpoint_filtered_keys:
          - vae.

dataset_config_name: default
do_train: True
do_val: True
do_test: False
checkpoint: null # /netscratch/saifullah/torchfusion/output/run/image_generation/pretrain_fp32/IITCDIP_kl_autoencoder_gan/default/checkpoints/checkpoint_103000.pt
checkpoint_state_dict_key: model
fid_stats: ${args.data_args.dataset_dir}/fid_stats/train-${args.data_args.dataset_statistics_n_samples}-stats.pth