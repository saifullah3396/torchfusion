experiment_name: default
clear_cuda_cache: true
cutmixup_args:
  cutmix: 0
  cutmix_minmax: null
  mixup: 0
  mixup_mode: batch
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
early_stopping_args:
  cumulative_delta: false
  min_delta: 0.0
  mode: min
  monitored_metric: null # val/loss
  patience: 5
enable_checkpointing: true
enable_grad_clipping: false
eval_training: true
eval_on_start: true
gradient_accumulation_steps: 2
log_gpu_stats: False
log_to_tb: true
logging_steps: 10
lr_schedulers:
  gen:
    name: cosine_annealing_lr
    params:
      eta_min: 1.0e-6
  disc:
    name: cosine_annealing_lr
    params:
      eta_min: 1.0e-6
eval_every_n_epochs: 1
max_epochs: 20
max_grad_norm: 1.0
min_epochs: null
model_checkpoint_config:
  dir: checkpoints
  every_n_epochs: null
  every_n_steps: 1000
  mode: min
  monitored_metric: validation/rec_loss
  n_best_saved: 3
  n_saved: 3
  name_prefix: ""
  save_weights_only: false
model_ema_args:
  enabled: true
  momentum: 0.0001
  update_every: 1
  momentum_warmup: 0.999 # start from online model
  warmup_iters: 0
non_blocking_tensor_conv: false
optimizers:
  gen:
    group_params:
      - group_name: gen
        kwargs:
          lr: 5.0e-5
          betas:
            - 0.5
            - 0.9
    name: adam
  disc:
    group_params:
      - group_name: disc
        kwargs:
          lr: 5.0e-5
          betas:
            - 0.5
            - 0.9
    name: adam
resume_checkpoint_file: null
resume_from_checkpoint: true
smoothing: 0.0
stop_on_nan: true
sync_batchnorm: true
test_checkpoint_file: null
warmup_ratio: 0.0 # 2 epochs
warmup_steps: 5000
wd_schedulers: null
with_amp: True
load_best_checkpoint_resume: false
outputs_to_metric:
  - ae_loss
  - kl_loss
  - nll_loss
  - rec_loss
  - g_loss
  - disc_loss
metric_args:
  - name: fid
    kwargs:
      fid_stats: ${fid_stats}