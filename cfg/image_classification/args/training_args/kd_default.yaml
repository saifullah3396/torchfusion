experiment_name: default
clear_cuda_cache: true
cutmixup_args:
  cutmix: 0
  cutmix_minmax: null
  mixup: 0
  mixup_mode: batch
  mixup_prob: 1.0
  mixup_switch_prob: 0.5
early_stopping_args:
  cumulative_delta: false
  min_delta: 0.0
  mode: min
  monitored_metric: null # val/loss
  patience: 5
enable_checkpointing: true
enable_grad_clipping: false
eval_on_start: true
gradient_accumulation_steps: 1
log_gpu_stats: False
log_to_tb: false
logging_steps: 100
lr_schedulers:
  default:
    name: multi_step_lr
    params:
      milestones: [60, 120, 160]
      gamma: 0.2
  # default:
  #   name: poly_decay_lr
  #   params:
  #     max_decay_steps: -1
  # default:
  #   name: step_lr
  #   params:
  #     step_size: 1
  # default:
  #   name: cosine_annealing_lr
  #   params: {}
  #   restarts: false
  # default:
  #   name: reduce_lr_on_plateau
  #   params:
  #     metric_name: val/loss
  #     save_history: true
  #     mode: min
  #     patience: 3
eval_every_n_epochs: 1
max_epochs: 100
max_grad_norm: 1.0
min_epochs: null
model_checkpoint_config:
  dir: checkpoints
  every_n_epochs: 1
  every_n_steps: null
  mode: max # min
  monitored_metric: validation/f1
  n_best_saved: 1
  n_saved: 1
  name_prefix: ""
  save_weights_only: false
model_ema_args:
  enabled: false
  momentum: 0.0001
  update_every: 1
  # momentum_warmup: 0.999 # start from online model
  # warmup_iters: 1000
non_blocking_tensor_conv: false
optimizers:
  # default:
  #   group_params:
  #     - group_name: default
  #       kwargs:
  #         betas:
  #           - 0.9
  #           - 0.999
  #         eps: 1.0e-08
  #         lr: 2.0e-05
  #         weight_decay: 0.0
  #   name: adam
  default:
    group_params:
      - group_name: default
        kwargs:
          lr: 1.0e-1
          momentum: 0.9
          nesterov: true
          weight_decay: 0.0005
    name: sgd
resume_checkpoint_file: null
resume_from_checkpoint: true
smoothing: 0.0
stop_on_nan: true
sync_batchnorm: true
test_checkpoint_file: null
warmup_ratio: 0 # 2 epochs
warmup_steps: 0
wd_schedulers: null
with_amp: False
load_best_checkpoint_resume: false
metric_args:
  - name: accuracy
    kwargs: {}
  - name: precision
    kwargs: 
      average: macro
  - name: recall
    kwargs: 
      average: macro
  - name: f1
    kwargs: {}
    
outputs_to_metric:
  - loss