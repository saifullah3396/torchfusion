# @package _global_
defaults:
  - /args/data_args: "???"
  - /args/model_args: knowledge_distillation/wide_residual_network
  - override /args/training_args: kd_default

args:
  general_args:
    do_train: ${do_train}
    do_val: ${do_val}
    do_test: ${do_test}
  model_args:
    kd_args:
      model_mode: ${model_mode}
  training_args:
    experiment_name: cifar10_train
    max_epochs: 200
    eval_on_start: True
    eval_every_n_epochs: 20
    test_run: False
  data_args:
    cache_file_name: default
    train_val_sampler: ${train_val_sampler}
    data_loader_args:
      per_device_train_batch_size: 128
      per_device_eval_batch_size: 128
      dataloader_drop_last: False
      shuffle_data: True
      pin_memory: True
      dataloader_num_workers: 8
      use_test_set_for_val: True

realtime_image_size_x: 32
realtime_image_size_y: 32
do_train: True
do_val: True
do_test: False
model_mode: '???'
train_val_sampler:
  name: RandomSplitSampler
  kwargs:
    random_split_ratio: 0.8 # 0.9 train / 0.1 val
    seed: 42